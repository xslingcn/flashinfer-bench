{
    "definition": {
      "name": "rmsnorm",
      "description": "Root Mean Square Normalization, a common layer normalization variant.",
      "axes": {
        "batch_size": {
          "type": "var"
        },
        "hidden_size": {
          "type": "const",
          "value": 4096
        }
      },
      "inputs": {
        "input": {
          "shape": [
            "batch_size",
            "hidden_size"
          ],
          "dtype": "float16"
        },
        "weight": {
          "shape": [
            "hidden_size"
          ],
          "dtype": "float16"
        },
        "eps": {
          "shape": [],
          "dtype": "float32"
        }
      },
      "outputs": {
        "output": {
          "shape": [
            "batch_size",
            "hidden_size"
          ],
          "dtype": "float16"
        }
      },
      "reference": "import torch\n\ndef run(input, weight, eps):\n    variance = input.to(torch.float32).pow(2).mean(-1, keepdim=True)\n    rstd = torch.rsqrt(variance + eps)\n    hidden_states = input * rstd\n    output = (hidden_states * weight).to(weight.dtype)\n    return {\"output\": output}"
    },
    "implementations": [
      {
        "name": "rmsnorm_triton_v1",
        "workload": "rmsnorm",
        "description": "A high-performance RMSNorm implementation using Triton. Generated by one-shot inquiry with Gemini-2.5-Pro.",
        "author": "gemini-2.5-pro-mystery-agent",
        "spec": {
          "language": "Triton",
          "target_hardware": [
            "NVIDIA_H100",
            "NVIDIA_A100",
            "NVIDIA_B200"
          ],
          "dependencies": [
            "torch",
            "triton >= 2.3"
          ],
          "entry_point": "run",
          "build_steps": []
        },
        "sources": [
          {
            "path": "main.py",
            "content": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _rmsnorm_kernel(x_ptr, weight_ptr, output_ptr,\n                    x_row_stride, output_row_stride,\n                    n_cols, eps, \n                    BLOCK_SIZE: tl.constexpr):\n    # Get the row index for this program instance\n    row_idx = tl.program_id(0)\n\n    # Create pointers to the beginning of the current row\n    row_x_ptr = x_ptr + row_idx * x_row_stride\n    row_output_ptr = output_ptr + row_idx * output_row_stride\n\n    # --- Pass 1: Calculate mean of squares ---\n    var_acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, n_cols, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < n_cols\n        # Load input data, converting to float32 for accumulation\n        x = tl.load(row_x_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n        var_acc += x * x\n    \n    # Reduce the block-level accumulators to a single scalar value for the row variance\n    row_var = tl.sum(var_acc, axis=0) / n_cols\n    rstd = tl.rsqrt(row_var + eps)\n\n    # --- Pass 2: Normalize and apply weight ---\n    for off in range(0, n_cols, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < n_cols\n\n        # Load input and weight\n        x = tl.load(row_x_ptr + cols, mask=mask, other=0.0)\n        w = tl.load(weight_ptr + cols, mask=mask, other=0.0)\n\n        # Normalize, apply weight, and store\n        x_normalized = x * rstd\n        output = x_normalized * w\n        tl.store(row_output_ptr + cols, output, mask=mask)\n\ndef run(input: torch.Tensor, weight: torch.Tensor, eps: float):\n    \"\"\"\n    Launcher function for the RMSNorm Triton kernel.\n\n    Args:\n        input (torch.Tensor): The input tensor of shape (batch_size, hidden_size).\n        weight (torch.Tensor): The weight tensor of shape (hidden_size).\n        eps (float): A small value to prevent division by zero.\n\n    Returns:\n        dict: A dictionary containing the output tensor under the key 'output'.\n    \"\"\"\n    # Ensure input tensor is contiguous in the last dimension\n    input = input.contiguous()\n    n_rows, n_cols = input.shape\n\n    # Create the output tensor\n    output = torch.empty_like(input)\n\n    # Pick a block size. 1024 is a good default for typical hidden sizes.\n    BLOCK_SIZE = 1024\n\n    # Define the grid for launching the kernel\n    # One program instance per row\n    grid = (n_rows,)\n\n    # Launch the kernel\n    _rmsnorm_kernel[grid](input, weight, output,\n                         input.stride(0), output.stride(0),\n                         n_cols, eps, \n                         BLOCK_SIZE=BLOCK_SIZE)\n    \n    return {\"output\": output}"
          }
        ],
        "evaluation_results": [
          {
            "eval_name": "rmsnorm_sglang_llama_sharegpt_h100",
            "status": "PASSED",
            "log_file": "https://hub.flashinfer.ai/gemini-2.5-pro-mystery-agent/rmsnorm_triton_v1/evaluations/rmsnorm_sglang_llama_sharegpt_h100_1751042700.log",
            "correctness": {
              "max_relative_error": 1.15e-05,
              "max_absolute_error": 0.89e-05
            },
            "performance": {
              "latency_ms": 0.008,
              "reference_latency_ms": 0.019,
              "speedup_factor": 2.375
            },
            "environment": {
              "device": "NVIDIA_H100",
              "libs": {
                "cuda": "12.6",
                "torch": "2.6.0",
                "triton": "2.4.0"
              }
            },
            "timestamp": "2025-06-27T12:45:00Z"
          }
        ]
      }
    ],
    "evaluations": [
      {
        "name": "rmsnorm_sglang_llama_sharegpt_h100",
        "workload": "rmsnorm",
        "description": "RMS Norm evaluation collected by gemini_evaluation_agent running Llama-3.1-8B on SGLang, H100, and ShareGPT iuputs.",
        "environment": {
          "device": "NVIDIA_H100",
          "libs": {
            "cuda": "12.6",
            "torch": "2.6.0",
            "sglang": "0.4.8"
          },
          "framework": "SGLang",
          "model": "meta-llama/Llama-3.1-8B"
        },
        "entries": [
          {
            "axes": {
              "batch_size": 4
            },
            "inputs": {
              "input": {
                "format": "safetensors",
                "path": "/blob/rmsnorm/rmsnorm_sglang_llama_sharegpt_h100/b4_input.safetensors",
                "tensor_key": "input"
              },
              "weight": {
                "format": "safetensors",
                "path": "/blob/rmsnorm/rmsnorm_sglang_llama_sharegpt_h100/rmsnorm_weight.safetensors",
                "tensor_key": "weight"
              }
            }
          },
          {
            "axes": {
              "batch_size": 32
            },
            "inputs": {
              "input": {
                "format": "safetensors",
                "path": "/blob/rmsnorm/rmsnorm_sglang_llama_sharegpt_h100/b32_input.safetensors",
                "tensor_key": "input"
              },
              "weight": {
                "format": "safetensors",
                "path": "/blob/rmsnorm/rmsnorm_sglang_llama_sharegpt_h100/rmsnorm_weight.safetensors",
                "tensor_key": "weight"
              }
            }
          },
          {
            "axes": {
              "batch_size": 128
            },
            "inputs": {
              "input": {
                "format": "safetensors",
                "path": "/blob/rmsnorm/rmsnorm_sglang_llama_sharegpt_h100/b128_input.safetensors",
                "tensor_key": "input"
              },
              "weight": {
                "format": "safetensors",
                "path": "/blob/rmsnorm/rmsnorm_sglang_llama_sharegpt_h100/rmsnorm_weight.safetensors",
                "tensor_key": "weight"
              }
            }
          }
        ]
      }
    ]
  }
  